---
title: "11-hwe-filtered-baseline-and-mixture"
output: html_notebook
---

24 February 2023

This relies on the locus evaluation in `10-locus-summary-and-eval.Rmd` and the initial analysis in `09-reference-baseline-mixture-analysis.Rmd`.

Implementing the functions for reading in genos from the rds files created by microhaplot.

I used `R-main/01-compile-and-tidy-rds-files.R` to generate an rds file with the genotype info in `data/processed`.

The R function uses a minimum read depth of 10 reads for the first allele and 6 reads for the second allele and a minimum allele balance of 0.4 (this can be modified in `R/microhaplot-genos-funcs.R`).

The VCF file used to generate these rds files is `BFAL_reference_filtered.vcf`, currently on Sedna. I might return to this by merging additional samples into that VCF file, but those would be bycatch - not reference samples - and the benefit to doing so is uncertain.

Read in the `loci_to_toss_hwe.csv` file and then everything should be straightforward and ported from the prior analysis.


```{r load-libraries}
library(tidyverse)
library(readxl)
library(stringr)
library(lubridate)
```


```{r}
# remove the loci that had deviations from HWE in the majority of populations
loci_to_toss_hwe <- read_csv("csv_outputs/loci_to_toss_hwe.csv")

# read in rds file with genotypes
genos_long <- read_rds("../data/processed/called_genos_na_explicit.rds") %>%
  anti_join(., loci_to_toss_hwe)
  
  
```

Add metadata
```{r}
# metadata for bycatch
# from Rmd 12-metadata
meta_w_ss <-  read_csv("../data/processed/metadata_w_samplesheet.csv")

meta_ids <- meta_w_ss %>%
  select(gtseq_run, id, `MWVCRC#`) %>%
  rename(sampleID = `MWVCRC#`) %>%
  mutate(population = NA) %>%
  select(sampleID, everything())


# metadata for reference samples used for lcWGS
meta1 <- readxl::read_xlsx("../data/BFAL_WGS_01_finalPlateMap.xlsx", sheet = "combinedPlate1And2SamplePops")

# from the MiSeq sample sheet
samplelist <- read_csv("../data/gtseq5_samplelist.csv") %>%
  mutate(gtseq_run = "gtseq5") %>%
  rename(id = sample)

# combine samplesheet and reference sample info
metadata <- samplelist %>%
  left_join(., meta1, by = c("Sample_ID" = "ind_ID")) %>%
  mutate(population = ifelse(!is.na(pop), pop, population)) %>%
  mutate(population = ifelse(is.na(population), "bycatch", population)) %>%
  select(-pop)

# change sampleID format
#metadata$Sample_ID <- gsub("BFAL", "", metadata$Sample_ID)

# metadata for Lehua
lehua <- metadata %>%
  filter(population == "Lehua")
lehua$Sample_ID <- gsub("BFAL", "", lehua$Sample_ID)


# just reference samples
ref_samples <- metadata %>%
  filter(!population %in% c("bycatch","NTC", "Lehua")) %>% # remove the 2 Lehua samples from the baseline
  select(Sample_ID, gtseq_run, id, population) %>%
  rename(sampleID = Sample_ID)


# need to fix the ids
ref_banded_birds <- ref_samples %>%
    filter(!str_detect(sampleID, "TOR")) %>%
    filter(!str_detect(sampleID, "KURE")) %>%
      filter(!str_detect(sampleID, "PCMB"))

ref_banded_birds$sampleID <- gsub("BFAL", "", ref_banded_birds$sampleID)


reference_samples <- ref_samples %>%
  anti_join(., ref_banded_birds, by = c("gtseq_run", "id")) %>%
  bind_rows(ref_banded_birds) %>%
  filter(sampleID != "21-0173") # remove the one Laysan bird that was a big outlier in the z-scores

# 137 samples included in the reference data  
ref_pops <- reference_samples %>%
  select(gtseq_run, id, sampleID, population) %>%
  unique()

```


Merge metadata and genotypes
```{r}
# merge reference metadata and the rest of the metadata
non_ref_ids <- meta_ids %>%
  anti_join(., reference_samples, by = c("sampleID"))

all_samples_w_ids <- bind_rows(reference_samples, non_ref_ids)

# are there duplicates there?
duplicates <- all_samples_w_ids %>%
  group_by(sampleID) %>%
  tally() %>%
  filter(n>1)

duplicates %>%
  left_join(., all_samples_w_ids) %>%
  filter(gtseq_run != "gtseq5") %>% # assuming those are banded birds
  arrange(sampleID) %>%
  group_by(sampleID) %>%
  tally() %>%
  filter(n>1) %>%
  select(sampleID) %>%
  left_join(., meta_w_ss, by = c("sampleID" = "MWVCRC#")) %>%
  write_csv("csv_outputs/duplicate_samples.csv")

```
These are the tissues for which we either a) got multiple muscle samples or b) genotyped inadvertantly twice, or c) mislabeled??



```{r}
meta_w_ss %>%
  filter(str_detect(`MWVCRC#`, "21-0318"))
```
Appears to be real... I can double check with Claire about whether that was intentional.


```{r}
genos_w_ids <- genos_long %>%
  left_join(., all_samples_w_ids, by = c("gtseq_run", "id")) %>%
  select(sampleID, everything())

```
Come back to figure out what's going on with the missing sample IDs


## Some initial filters

### Take highest read-depth call for multiply-genotyped DNA_IDs

I'm not sure if there are any of these, but best to leave it in here...

Now, here is a harder operation: if an individual is multiply-genotyped, take the
genotype with the highest total read depth.  
```{r take-just-one}
# slow-ish function to get the total read depth column
tdepth <- function(a, d) {
  if(any(is.na(a))) {
    return(NA)
  }
  if(a[1]==a[2]) {
    return(d[1])
  } else {
    return(d[1] + d[2])
  }
  
}
# this takes the highest read-depth instance of each duplicately-genotyped individual.
geno_one_each <- genos_w_ids %>%
  group_by(sampleID, locus, gtseq_run) %>%
  mutate(total_depth = tdepth(allele, depth)) %>%
  ungroup() %>%
  arrange(sampleID, locus, total_depth, gtseq_run, depth) %>%
  group_by(sampleID, locus) %>%
  mutate(rank = 1:n()) %>% # this ranks all alleles for a given individual by depth
  ungroup() %>%
  filter(rank <= 2) # this takes the top ranked alleles and should remove duplicates
  

# how many samples now?
geno_one_each %>%
  filter(!is.na(sampleID)) %>%
  group_by(sampleID) %>% 
  select(sampleID, gtseq_run, id) %>%
  unique() %>%
  tally() %>%
  filter(n>1) 

```
949 unique sampleIDs; 38 samples were genotyped twice.

Because there are multiple gtseq runs/ids for the same individual, I should pivot to using the sampleID as the unique key, and there is only one complete set of alleles for each sampleID.


```{r}
# try using this df to eliminate issues with duplication
genos_no_dups <- geno_one_each %>%
  select(-gtseq_run, -id, -total_depth, -rank)

```






I need to vet both the loci and the individuals for missing data, etc.

## Locus evaluation

How many loci and how many alleles?

```{r}
# alleles
genos_no_dups %>%
  filter(!is.na(allele)) %>%
  select(locus, allele) %>%
  unique() %>%
  arrange(locus)

# loci
genos_no_dups %>%
  filter(!is.na(allele)) %>%
  select(locus, allele) %>%
  unique() %>%
  group_by(locus) %>%
  tally() %>%
  arrange(desc(n))

  
```

466 alleles across 185 loci with between 1-8 alleles per locus.

Missing data:

949 individuals * 2 alleles per locus = 1898 alleles per locus

50% missing data per locus = 949
```{r}
# missing data across loci
locs_to_toss <- genos_no_dups %>%
  group_by(locus) %>%
  mutate(missingness = ifelse(is.na(allele), 1, 0)) %>%
  summarise(sum(missingness)) %>% # 190 loci x2 = total
  filter(`sum(missingness)`>949) %>% # more than 50% missing data
  select(locus) # drop those loci for now and see how the assignment goes

# just the keepers
genos_locs_filtered <- genos_no_dups %>%
  anti_join(., locs_to_toss)

```




```{r}
# summary of remaining loci
genos_locs_filtered %>%
  filter(!is.na(allele)) %>%
  select(locus, allele) %>%
  unique()

genos_locs_filtered %>%
  filter(!is.na(allele)) %>%
  select(locus, allele) %>%
  unique() %>%
  group_by(locus) %>%
  tally() %>%
  arrange(desc(n))

```

444 alleles in 178 loci with 1-8 alleles per locus.


First, look at the loci for missingness and \>2 haplotypes in an individual [This might be masked by the function that reads in the rds file??]

```{r}
genos_no_dups %>%
  group_by(sampleID, locus) %>% # there should be no more than 2 alleles for a given indiv/locus
  tally() %>%
  filter(n > 2)

```

## Missing data in individuals

Total number of loci = 178 Total number of gene copies = 356

Total number of samples = 949

```{r}
inds_to_toss <- genos_locs_filtered %>%
  group_by(sampleID) %>%
  mutate(missingness = ifelse(is.na(allele), 1, 0)) %>%
  summarise(sum(missingness)) %>%
  arrange(desc(`sum(missingness)`)) %>%
  filter(`sum(missingness)` > 89) # remove samples with >25% missing data

# just the keepers
genos_locs_ind_filtered <- genos_locs_filtered %>%
  anti_join(., inds_to_toss)

```

There's a lot of missing data, unfortunately. We might use a higher threshold for missing data for loci to retain more individuals if possible.

25% missing data threshold = drop 61 indivs; keep 889.

```{r}
883/949
```

Take a look at that dataset

```{r}
genos_locs_ind_filtered  %>%
  ggplot(aes(x = reorder(locus, depth), y = reorder(sampleID, depth), fill = log10(depth))) +
  geom_tile()

```

## self-assignment

Doing a sanity check with the reference baseline

```{r}
# first make integers of the alleles
alle_idxs <- genos_locs_ind_filtered %>% 
  filter(!is.na(sampleID)) %>%
  dplyr::select(sampleID, locus, gene_copy, allele) %>%
  group_by(locus) %>%
  mutate(alleidx = as.integer(factor(allele, levels = unique(allele)))) %>%
  ungroup() %>%
  arrange(sampleID, locus, alleidx) # rubias can handle NA's, so no need to change them to 0's

```



```{r}
# get the allele indexes for just the reference populations
alle_idxs %>%
  select(sampleID) %>%
  unique() %>%
  inner_join(., ref_pops) %>%
  left_join(., alle_idxs)

alle_idxs %>%
  group_by(sampleID) %>%
  tally()

```

```{r}
# format for rubias
reference <- alle_idxs %>%
  inner_join(., ref_pops) %>%
  select(-allele, -gtseq_run, -id) %>%
  select(population, sampleID, everything()) %>%
  rename(collection = population, indiv = sampleID)

# make two-col format
ref_two_col <- reference %>%
  unite("loc", 3:4, sep = ".") %>%
  pivot_wider(names_from = loc, values_from = alleidx) %>%
  mutate(repunit = collection) %>%
  mutate(sample_type = "reference") %>%
  select(sample_type, repunit, collection, everything()) %>% # modify repunit info for Whale-Skate and Tern, which should be a single repunit for the French Frigate Shoals
  mutate(repunit = ifelse(collection %in% c("Tern", "Whale-Skate"), "FFS", repunit))

```



### Self-assignment of colony samples

KEY CAVEAT: I cannot assess the success of assignment using the same samples I used to design the loci for the GTseq panel. Therefore, what I will do is use banded birds as a hold-out dataset and the samples from my reference populations as the training dataset.

Another point for consideration is the discrepancy between sample sizes for the colonies. Depending on how the hold-out dataset works, I can revisit downsampling some of the populations to bring them closer together.


```{r}
ref_two_col %>%
  group_by(repunit) %>%
  tally()

```
A little background here:

Rather than using self-assignment, I need to look at the leave-one-out assessment because I know that doing self-assignment with my reference being identical to my ascertainment samples is going to be upwardly biased.


```{r}
library(rubias)

loo_output <- assess_reference_loo(ref_two_col, gen_start_col = 5, return_indiv_posteriors = T)

# top p of z for each simulated individual
loo_top_pofz <- loo_output$indiv_posteriors %>%
  group_by(repunit_scenario, collection_scenario, iter, indiv, simulated_repunit, simulated_collection) %>%
  slice_max(., order_by = PofZ)

loo_top_pofz %>%
  filter(simulated_repunit != repunit &
           PofZ > 0.9)

405/5000
158/5000
```
overall: 405 rows out of 5000 (8% mis-assignments)
at the 90% probability threshold: 158/5000 3% misassignments 


```{r loo-mixing-proportions}
# loo_output %>%
#   group_by(repunit, iter) %>%
#   summarise(sum_true = sum(true_pi), sum_inferred= sum(post_mean_pi))
```
compare hard-calling to mixing proportions








```{r}
# self-assignment
baseline_assign <- rubias::self_assign(ref_two_col, gen_start_col = 5)

```

```{r}
top_assign <- baseline_assign %>%
  group_by(indiv) %>%
  slice_max(., order_by = scaled_likelihood) 

top_assign %>% # top assignment for each sample
  ggplot(aes(x = scaled_likelihood)) +
  geom_histogram()

```

Looks like below 90% would be a reasonable cut-off?

```{r}
top_assign %>%
  filter(repunit != inferred_repunit)

top_assign %>%
  filter(repunit != inferred_repunit &
         scaled_likelihood > 0.9)

```

7 mis-assignments, but only 4 with a scaled-likelihood > 0.9.


So overall:

```{r}
# correct assignment with no likelihood threshold?
top_assign %>%
  filter(repunit == inferred_repunit)

119/126

top_assign %>%
  filter(scaled_likelihood > 0.9 &
         repunit == inferred_repunit) 
```

126 samples in the reference baseline (11 dropped out bec of missing data).

94% of samples correctly assigned at any scaled likelihood.

```{r}
119/126
```

If we only look at samples that were assigned at \> 90% threshold:

```{r}

mypalette <- c("#006d2c", "#31a354", "#74c476",  "skyblue", "darkslateblue", "dodgerblue", # Hawaii - greens, blues
               "tomato") # Japan - red
```


```{r}
top_assign %>%
  filter(scaled_likelihood > 0.9) %>%
  group_by(repunit, inferred_repunit) %>%
  tally() %>%
  ggplot(aes(x = repunit, y = n, fill = inferred_repunit)) +
  geom_bar(stat = "identity", position = "stack") +
  theme_minimal() +
  labs(title = "Self-assignment of reference birds to baseline, 90% probability",
       y = "Samples",
       x = "Baseline reporting unit",
       fill = "Inferred reporting unit") +
  scale_fill_manual(values = c("#74c476", "dodgerblue", "darkgreen", "darkslateblue", "tomato")) +
  theme(
    axis.title.x = element_text(margin = margin(t=10)),
      axis.title.y = element_text(margin = margin(r = 10))
  )
  

ggsave("pdf_outputs/AscertainmentSamples_self_assignment90.pdf", width = 8, height = 5)
  
  
  

top_assign %>%
  filter(scaled_likelihood > 0.9 &
           repunit == inferred_repunit)

116/120

```

Which would be 97% accurate assignment... but again, that is using the ascertainment samples. 

Quick look at z-scores:

```{r}
top_assign %>%
  ggplot(aes(z_score)) +
  geom_histogram()

```

Woah. There's one outlier.

```{r}
top_assign %>%
  filter(z_score < -3)

```

Maybe a different species accidentally? I can double-check the sample number with metadata that Jessie sent. Other than that, things are looking good to proceed with mixture assignment.

Also, Jessie mentioned hybrids between Laysan and BFAL...

```{r}
# correct number of alleles?
alle_idxs %>%
  inner_join(., ref_pops) %>%
  group_by(sampleID) %>%
  tally() %>%
  filter(n >356)

```




## Hold out dataset


```{r}
# get the format right
mix_two_col <- alle_idxs %>%
  filter(!is.na(sampleID)) %>% # remove any NAs at this stage
  anti_join(., ref_pops) %>%
  anti_join(., lehua, by = c("sampleID" = "Sample_ID")) %>%
  select(-allele) %>%
  unite("loc", 2:3, sep = ".") %>%
  pivot_wider(names_from = loc, values_from = alleidx) %>%
  mutate(repunit = NA) %>%
  mutate(sample_type = "mixture") %>%
  mutate(collection = "bycatch") %>%
  select(sample_type, repunit, collection, everything())

mix_two_col$repunit <- as.character(mix_two_col$repunit)
head(mix_two_col)

```

Figure out which of those sampleIDs correspond to banded birds for the hold-out dataset

```{r}
# get the sample ids for the hold-out data
banded_ids_locs <- meta_w_ss %>%
  filter(`BandY/N` == "TRUE") %>%
  rename(sampleID = `MWVCRC#`) %>%
  select(sampleID, `Loc-Abbr`) %>%
  filter(!is.na(`Loc-Abbr`)) %>%
  anti_join(., reference, by = c("sampleID" = "indiv")) %>%
  filter(`Loc-Abbr` != "Kauai, Kilauea")
  

# finalize the formating and location info
hold_out_2col <- mix_two_col %>%
  inner_join(., banded_ids_locs) %>%
  rename(indiv = sampleID) %>%
  mutate(collection = `Loc-Abbr`) %>%
  select(-`Loc-Abbr`) %>%
  group_by(indiv) %>%
  slice_sample(n = 1) %>%
  ungroup()
  
# confirm there are no duplicate sample ids
hold_out_2col %>%
  group_by(indiv) %>%
  tally() %>%
  filter(n >1) %>%
  left_join(., hold_out_2col)

# for whatever reason, rubias is being finicky about the format and it's easiest to do this
# to make sure both df are consistent
tmp <- bind_rows(ref_two_col, hold_out_2col)

mix_holdout <- tmp %>%
  filter(sample_type == "mixture")

ref <- tmp %>%
  filter(sample_type == "reference")
```
759 samples in this mixture dataframe
174 banded birds with location information

62 banded birds in the hold out dataset.



```{r}
# mixture analysis for the hold out dataset
# holdout_assigned <- rubias::infer_mixture(reference = ref, mixture = mix_holdout, gen_start_col = 5)


```
### Results from holdout dataset

```{r}
# mixing proportions for all results
holdout_assigned$mixing_proportions %>%
  group_by(mixture_collection, repunit) %>%
  summarise(sum_pi = sum(pi)) %>%
  ggplot(aes(x = repunit, y = sum_pi, fill = mixture_collection)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
    scale_fill_manual(values = c("#74c476", "dodgerblue", "darkgreen", "darkslateblue", "tomato"))

```

Look at individual assignments - and then filter individuals before summarizing the mixing proportions
```{r}
# what about z-scores, we'd expect some outliers
top_holdout_assign <- holdout_assigned$indiv_posteriors %>%
  group_by(indiv) %>%
  slice_max(., order_by = PofZ)

top_holdout_assign %>%
  filter(z_score < -3 | z_score > 3) 

```
No z-score outliers. Great.


What are the expected proportions, so I can generate a plot of expected vs. observed
```{r}
holdout_n <- mix_holdout %>%
  group_by(collection) %>%
  tally() %>%
  rename(expected = n) %>%
  separate(collection, into = c("NWHI", "repunit"))

```

```{r}
# what about using the individual assignments?
holdout_results <- top_holdout_assign %>%
  group_by(repunit) %>%
  tally() %>%
  rename(observed = n)

holdout_n %>%
  left_join(., holdout_results) %>%
  pivot_longer(cols = 3:4, names_to = "type", values_to = "samples") %>%
  ggplot(aes(x = repunit, y = samples, fill = type)) +
  geom_bar(position = "dodge", stat = "identity") +
  theme_minimal() 

```

What about using a 90% threshold?

48/62 at that threshold

```{r}
top_holdout_assign %>%
  filter(PofZ > 0.9) %>%
  group_by(mixture_collection, repunit) %>%
  tally() %>%
  ggplot(aes(x = mixture_collection, y = n, fill = repunit)) +
  geom_bar(stat = "identity", position = "stack") +
  theme_minimal() +
  labs(title = "Assignment of banded birds (hold-out dataset) to baseline, 90% probability",
       y = "Samples",
       x = "Banding location",
       fill = "Reporting unit") +
    scale_fill_manual(values = c("#74c476", "dodgerblue", "darkgreen", "darkslateblue", "tomato")) +
  theme(
    axis.title.x = element_text(margin = margin(t=10)),
      axis.title.y = element_text(margin = margin(r = 10))
  )
  

ggsave("pdf_outputs/holdout_dataset_assignment90.pdf", width = 8, height = 5)
```
```{r}
top_holdout_assign %>%
  filter(PofZ > 0.9) %>%
  group_by(mixture_collection, repunit) %>%
  tally() 


```

```{r}
holdout_results <- top_holdout_assign %>%
  separate(mixture_collection, into = c("isl", "location"), sep = ", ")

ho_results <- holdout_results %>%
  filter(PofZ > 0.9) %>%
  group_by(location, repunit) %>%
  tally() %>%
  ungroup() %>%
  group_by(location) %>%
  mutate(total = sum(n)) %>%
  mutate(correct = ifelse(location == repunit, n/total, 0)) %>%
  ungroup() %>%
  filter(repunit == location)  %>%
  select(location, repunit, correct)
  
ho_results
```



Until I figure out exactly what to do with the mixing proportions, I'll go forward with comparing the expected vs. observed individual assignments:

Next steps:
1. Play with mixing proportions
2. test different samples per population in the baseline




### Downsample the baseline

```{r}
# set.seed(731)
# 
# biguns <- ref_two_col %>%
#   filter(repunit %in% c("FFS", "Kure", "Torishima"))
# 
# downsampled_bigs <- biguns %>%
#   group_by(repunit) %>%
#   sample_n(20, replace = F) %>% # the Laysan sample size is so small!!
#   ungroup()
# 
# downsampled_baseline <- ref_two_col %>%
#   anti_join(., biguns) %>%
#   bind_rows(downsampled_bigs)
```

Now try using the downsampled baseline for the hold-out dataset:

```{r}
# mixture analysis for the hold out dataset
# holdout_downsamp <- rubias::infer_mixture(reference = downsampled_baseline, mixture = mix_holdout, gen_start_col = 5)


```

```{r}
# what about z-scores, we'd expect some outliers
# top_holdout_downsamp <- holdout_downsamp$indiv_posteriors %>%
#   group_by(indiv) %>%
#   slice_max(., order_by = PofZ)
# 
# top_holdout_downsamp %>%
#   filter(z_score < -3 | z_score > 3) 
# 
# top_holdout_downsamp %>%
#   filter(PofZ > 0.9) %>%
#   group_by(mixture_collection, repunit) %>%
#   tally() %>%
#   ggplot(aes(x = mixture_collection, y = n, fill = repunit)) +
#   geom_bar(stat = "identity", position = "stack") +
#   theme_minimal() +
#   labs(title = "Assignment of banded birds (hold-out dataset) to baseline, 90% probability",
#        y = "Samples",
#        x = "Banding location",
#        fill = "Reporting unit") +
#     scale_fill_manual(values = c("#74c476", "dodgerblue", "darkgreen", "darkslateblue", "tomato")) +
#   theme(
#     axis.title.x = element_text(margin = margin(t=10)),
#       axis.title.y = element_text(margin = margin(r = 10))
#   )
  
```

Evaluate by looking at correct % and then pick a path forward.



```{r}
# downsamp_results <- top_holdout_downsamp %>%
#   separate(mixture_collection, into = c("isl", "location"), sep = ", ")
# 
# ds_results <- downsamp_results %>%
#   filter(PofZ > 0.9) %>%
#   group_by(location, repunit) %>%
#   tally() %>%
#   ungroup() %>%
#   group_by(location) %>%
#   mutate(total = sum(n)) %>%
#   mutate(correct = ifelse(location == repunit, n/total, 0)) %>%
#   ungroup() %>%
#   filter(repunit == location) %>%
#   rename(downsampled_correct = correct) %>%
#   select(location, repunit, downsampled_correct)
  

```


Comparison of downsampled results with original results
```{r}
# ho_results %>%
#   left_join(., ds_results)

```


As expected, the bias is toward the FFS when there are many more samples in that reporting group. 
I do wonder if having additional loci would be beneficial...









## Mixture bycatch assignment


```{r}
# for whatever reason, rubias is being finicky about the format and it's easiest to do this
# to make sure both df are consistent
mix2col <- mix_two_col %>%
  rename(indiv = sampleID)
tmp_combo <- bind_rows(ref_two_col, mix2col)

mix <- tmp_combo %>%
  filter(sample_type == "mixture")

ref <- tmp_combo %>%
  filter(sample_type == "reference")
```

```{r}
# mixture analysis
bycatch_assign <- rubias::infer_mixture(reference = ref, mixture = mix, gen_start_col = 5)

```

Bycatch results for mixture

```{r}
mix_assigned <- bycatch_assign$indiv_posteriors %>%
  group_by(indiv) %>%
  slice_max(., order_by = PofZ)


# distribution of PofZ for top assignments
mix_assigned %>%
  ggplot(aes(x = PofZ)) +
  geom_histogram()

```

759 bycatch samples, most of which are assigned at high probability.


Out of curiosity, take a look at the birds that are not BFAL:
```{r}
mix_assigned %>%
  filter(indiv %in% c("11-0626",
"11-0628",
"16-0412",
"16-0413",
"19-0314",
"12-0369",
"17-0268"))

```


```{r}
mix_assigned %>%
  filter(PofZ > 0.9)

```

This is where there's an interesting trade-off between dropping individuals from the analysis vs. entertaining the possibility of incorrect assignments. We can explore this further by looking at different thresholds in the self-assignment. We can also combine Whale-Skate and Tern into a single FFS reporting unit and maybe remove Lehua if it has too little data...

Quick summary for 90% likelihood

```{r}
mix_assigned %>%
  filter(PofZ > 0.9) %>%
  group_by(repunit) %>%
  tally()

```

```{r}
# how many samples are assigned to which reporting unit at what PofZ?
mix_assigned %>%
  ggplot(aes(x = PofZ, y = repunit)) +
  geom_point()

```
Filter for z-scores:
```{r}
mix_assigned %>%
  ggplot(aes(x = z_score)) +
  geom_histogram() 

```
There are barely any outliers - maybe because the input data is better organized, with NAs explicit, etc.?


```{r}
# z-score outliers
mix_assigned %>%
  filter(z_score < -3) %>%
  ggplot(aes(x = z_score)) +
  geom_histogram() 

```
The normal distribution looks asymmetrical: +2.5 on the right and -4 on the left...



```{r}
# z-score outliers
mix_assigned %>%
  filter(z_score < -3)

```
Laysan is clearly under-represented in the baseline - and that results in underestimate of proportion from the holdout data.

Additional notes:
We didn't have Pearl and Hermes - closer to Laysan - some of these bycatch could be from this colony, which has 1000s of birds, bigger than Laysan.


```{r}
mix_assigned %>%
  filter(z_score < -4)

```

A fair number of z-score outliers:
26 total.


Let's remove them for now and then assess the remaining bycatch samples/assignment results:
```{r}
# remove outliers
assigned_z_removed <- mix_assigned %>%
  filter(z_score > -3 & z_score < 3) 

assigned_z_removed %>%
  ggplot(aes(x = PofZ, y = repunit, color = collection)) +
  geom_jitter() +
  theme_minimal()

```
740 samples


Thinking about how to do some type of accumulation curve? How many samples are included at what level of confidence?

```{r}
assigned_z_removed %>%
  filter(PofZ > 0.9) %>%
  group_by(repunit) %>%
  tally()
```

x = PofZ
y = number of samples
color = reporting unit

Basically, I want to visualize the trade-off between type I and type II error.


## Add bycatch metadata


```{r}
#metadata_ss <- read_csv("../data/processed/metadata_w_samplesheet.csv")

```


Generate a full dataset to share with Jessie...
```{r}
# just the reference sample birds that were from Jessie's collections
tmp.samp <- top_assign %>%
  filter(!str_detect(indiv, "TOR")) %>%
    filter(!str_detect(indiv, "KURE")) %>%
      filter(!str_detect(indiv, "PCMB")) %>%
  select(-missing_loci)


bycatch_w_meta <- meta_w_ss %>% 
  #unite(col = "indiv", gtseq_run, id, sep = "_") %>%
  left_join(., assigned_z_removed, by = c("MWVCRC#" = "indiv")) %>%
  select(-missing_loci) %>%
  select(-c(20:28)) %>% # remove duplicates
  unique() %>%
  filter(!is.na(mixture_collection))

# confirm that duplicates are removed
bycatch_w_meta %>%
  group_by(`MWVCRC#`) %>%
  tally() %>%
  filter(n >1) # no duplicates
  
  
```

Laysan is clearly under-represented in the baseline. I might need to modify the baseline to account for that.

Okay, next steps:

1. Implement analysis function to deal with multiply-genotyped individuals. [DONE]
2. Revise the baseline to account for un-even numbers of individuals (Laysan under-represented).
3. Use banded birds that were not part of the lcWGS ascertainment panel for hold-out dataset to evaluate assignment success.[DONE]









```{r}
# output
bycatch_w_meta %>%
    write_csv("../data/processed/bycatch_assignments_w_metadata.csv")


# reference data
tmp.samp %>%
  left_join(., meta_w_ss, by = c("indiv" = "MWVCRC#")) %>%
  write_csv("../data/processed/reference_banded_assignments_w_metadata.csv")
```



